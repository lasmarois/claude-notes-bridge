program(1.0)
[buildInfo = dict<tensor<string, []>, tensor<string, []>>({{"coremlc-component-MIL", "3304.5.2"}, {"coremlc-version", "3304.6.2"}, {"coremltools-component-torch", "2.0.0"}, {"coremltools-version", "6.3.0"}})]
{
    func main<ios15>(tensor<fp32, [1, 512]> attention_mask, tensor<fp32, [1, 512]> input_ids) {
            tensor<int32, []> var_8 = const()[name = tensor<string, []>("op_8"), val = tensor<int32, []>(-1)];
            tensor<int32, [1]> var_33_axes_0 = const()[name = tensor<string, []>("op_33_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<string, []> attention_mask_to_fp16_dtype_0 = const()[name = tensor<string, []>("attention_mask_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<fp16, [1, 512]> cast_193 = cast(dtype = attention_mask_to_fp16_dtype_0, x = attention_mask);
            tensor<fp16, [1, 1, 512]> var_33_cast = expand_dims(axes = var_33_axes_0, x = cast_193);
            tensor<int32, [1]> var_34_axes_0 = const()[name = tensor<string, []>("op_34_axes_0"), val = tensor<int32, [1]>([2])];
            tensor<fp16, [1, 1, 1, 512]> var_34_cast = expand_dims(axes = var_34_axes_0, x = var_33_cast);
            tensor<fp16, []> var_13_to_fp16 = const()[name = tensor<string, []>("op_13_to_fp16"), val = tensor<fp16, []>(0x1p+0)];
            tensor<fp16, [1, 1, 1, 512]> var_37_cast = sub(x = var_13_to_fp16, y = var_34_cast);
            tensor<string, []> var_37_cast_to_fp32_dtype_0 = const()[name = tensor<string, []>("op_37_cast_to_fp32_dtype_0"), val = tensor<string, []>("fp32")];
            tensor<fp32, []> var_38 = const()[name = tensor<string, []>("op_38"), val = tensor<fp32, []>(-0x1.fffffep+127)];
            tensor<fp32, [1, 1, 1, 512]> cast_190 = cast(dtype = var_37_cast_to_fp32_dtype_0, x = var_37_cast);
            tensor<fp32, [1, 1, 1, 512]> attention_mask_1 = mul(x = cast_190, y = var_38);
            tensor<string, []> cast_0_dtype_0 = const()[name = tensor<string, []>("cast_0_dtype_0"), val = tensor<string, []>("int32")];
            tensor<int32, []> inputs_embeds_axis_0 = const()[name = tensor<string, []>("inputs_embeds_axis_0"), val = tensor<int32, []>(0)];
            tensor<fp16, [30522, 384]> model_embeddings_word_embeddings_weight_to_fp16 = const()[name = tensor<string, []>("model_embeddings_word_embeddings_weight_to_fp16"), val = tensor<fp16, [30522, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64)))];
            tensor<int32, [1, 512]> cast_189 = cast(dtype = cast_0_dtype_0, x = input_ids);
            tensor<fp16, [1, 512, 384]> inputs_embeds_cast = gather(axis = inputs_embeds_axis_0, indices = cast_189, x = model_embeddings_word_embeddings_weight_to_fp16);
            tensor<fp16, [1, 512, 384]> token_type_embeddings_1_to_fp16 = const()[name = tensor<string, []>("token_type_embeddings_1_to_fp16"), val = tensor<fp16, [1, 512, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23441024)))];
            tensor<fp16, [1, 512, 384]> embeddings_1_cast = add(x = inputs_embeds_cast, y = token_type_embeddings_1_to_fp16);
            tensor<fp16, [1, 512, 384]> position_embeddings_1_to_fp16 = const()[name = tensor<string, []>("position_embeddings_1_to_fp16"), val = tensor<fp16, [1, 512, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23834304)))];
            tensor<fp16, [1, 512, 384]> input_5_cast = add(x = embeddings_1_cast, y = position_embeddings_1_to_fp16);
            tensor<int32, [1]> input_7_axes_0 = const()[name = tensor<string, []>("input_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_embeddings_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_embeddings_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24227584)))];
            tensor<fp16, [384]> model_embeddings_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_embeddings_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24228416)))];
            tensor<fp16, []> var_10_to_fp16 = const()[name = tensor<string, []>("op_10_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 512, 384]> input_7_cast = layer_norm(axes = input_7_axes_0, beta = model_embeddings_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_embeddings_LayerNorm_weight_to_fp16, x = input_5_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24229248)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24524224)))];
            tensor<fp16, [1, 512, 384]> x_9_cast = linear(bias = model_encoder_layer_0_attention_self_query_bias_to_fp16, weight = model_encoder_layer_0_attention_self_query_weight_to_fp16, x = input_7_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24525056)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24820032)))];
            tensor<fp16, [1, 512, 384]> x_1_cast = linear(bias = model_encoder_layer_0_attention_self_key_bias_to_fp16, weight = model_encoder_layer_0_attention_self_key_weight_to_fp16, x = input_7_cast);
            tensor<int32, [4]> var_93 = const()[name = tensor<string, []>("op_93"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_3_cast = reshape(shape = var_93, x = x_1_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24820864)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25115840)))];
            tensor<fp16, [1, 512, 384]> x_5_cast = linear(bias = model_encoder_layer_0_attention_self_value_bias_to_fp16, weight = model_encoder_layer_0_attention_self_value_weight_to_fp16, x = input_7_cast);
            tensor<int32, [4]> var_102 = const()[name = tensor<string, []>("op_102"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_7_cast = reshape(shape = var_102, x = x_5_cast);
            tensor<int32, [4]> var_104 = const()[name = tensor<string, []>("op_104"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_108 = const()[name = tensor<string, []>("op_108"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_11_cast = reshape(shape = var_108, x = x_9_cast);
            tensor<bool, []> attention_scores_1_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_1_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_1_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_6_perm_0 = const()[name = tensor<string, []>("transpose_6_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_7_perm_0 = const()[name = tensor<string, []>("transpose_7_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp16, [1, 12, 32, 512]> transpose_39 = transpose(perm = transpose_7_perm_0, x = x_3_cast);
            tensor<fp16, [1, 12, 512, 32]> transpose_40 = transpose(perm = transpose_6_perm_0, x = x_11_cast);
            tensor<fp16, [1, 12, 512, 512]> attention_scores_1_cast = matmul(transpose_x = attention_scores_1_transpose_x_0, transpose_y = attention_scores_1_transpose_y_0, x = transpose_40, y = transpose_39);
            tensor<fp16, []> _inversed_attention_scores_3_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_attention_scores_3_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 512, 512]> _inversed_attention_scores_3_cast = mul(x = attention_scores_1_cast, y = _inversed_attention_scores_3_y_0_to_fp16);
            tensor<fp16, [1, 12, 512, 512]> input_11_cast = add(x = _inversed_attention_scores_3_cast, y = cast_193);
            tensor<fp16, [1, 12, 512, 512]> input_13_cast = softmax(axis = var_8, x = input_11_cast);
            tensor<bool, []> context_layer_1_transpose_x_0 = const()[name = tensor<string, []>("context_layer_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_1_transpose_y_0 = const()[name = tensor<string, []>("context_layer_1_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> transpose_41 = transpose(perm = var_104, x = x_7_cast);
            tensor<fp16, [1, 12, 512, 32]> context_layer_1_cast = matmul(transpose_x = context_layer_1_transpose_x_0, transpose_y = context_layer_1_transpose_y_0, x = input_13_cast, y = transpose_41);
            tensor<int32, [4]> var_120 = const()[name = tensor<string, []>("op_120"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_125 = const()[name = tensor<string, []>("op_125"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> transpose_38 = transpose(perm = var_120, x = context_layer_1_cast);
            tensor<fp16, [1, 512, 384]> input_15_cast = reshape(shape = var_125, x = transpose_38);
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25116672)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25411648)))];
            tensor<fp16, [1, 512, 384]> input_17_cast = linear(bias = model_encoder_layer_0_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_0_attention_output_dense_weight_to_fp16, x = input_15_cast);
            tensor<fp16, [1, 512, 384]> input_19_cast = add(x = input_17_cast, y = input_7_cast);
            tensor<int32, [1]> input_21_axes_0 = const()[name = tensor<string, []>("input_21_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25412480)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25413312)))];
            tensor<fp16, [1, 512, 384]> input_21_cast = layer_norm(axes = input_21_axes_0, beta = model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16, x = input_19_cast);
            tensor<fp16, [1536, 384]> model_encoder_layer_0_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25414144)))];
            tensor<fp16, [1536]> model_encoder_layer_0_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(26593856)))];
            tensor<fp16, [1, 512, 1536]> input_23_cast = linear(bias = model_encoder_layer_0_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_0_intermediate_dense_weight_to_fp16, x = input_21_cast);
            tensor<string, []> input_25_mode_0 = const()[name = tensor<string, []>("input_25_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_25_cast = gelu(mode = input_25_mode_0, x = input_23_cast);
            tensor<fp16, [384, 1536]> model_encoder_layer_0_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(26596992)))];
            tensor<fp16, [384]> model_encoder_layer_0_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27776704)))];
            tensor<fp16, [1, 512, 384]> input_27_cast = linear(bias = model_encoder_layer_0_output_dense_bias_to_fp16, weight = model_encoder_layer_0_output_dense_weight_to_fp16, x = input_25_cast);
            tensor<fp16, [1, 512, 384]> input_29_cast = add(x = input_27_cast, y = input_21_cast);
            tensor<int32, [1]> input_31_axes_0 = const()[name = tensor<string, []>("input_31_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_0_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27777536)))];
            tensor<fp16, [384]> model_encoder_layer_0_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27778368)))];
            tensor<fp16, [1, 512, 384]> input_31_cast = layer_norm(axes = input_31_axes_0, beta = model_encoder_layer_0_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_0_output_LayerNorm_weight_to_fp16, x = input_29_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27779200)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28074176)))];
            tensor<fp16, [1, 512, 384]> x_21_cast = linear(bias = model_encoder_layer_1_attention_self_query_bias_to_fp16, weight = model_encoder_layer_1_attention_self_query_weight_to_fp16, x = input_31_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28075008)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28369984)))];
            tensor<fp16, [1, 512, 384]> x_13_cast = linear(bias = model_encoder_layer_1_attention_self_key_bias_to_fp16, weight = model_encoder_layer_1_attention_self_key_weight_to_fp16, x = input_31_cast);
            tensor<int32, [4]> var_170 = const()[name = tensor<string, []>("op_170"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_15_cast = reshape(shape = var_170, x = x_13_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28370816)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28665792)))];
            tensor<fp16, [1, 512, 384]> x_17_cast = linear(bias = model_encoder_layer_1_attention_self_value_bias_to_fp16, weight = model_encoder_layer_1_attention_self_value_weight_to_fp16, x = input_31_cast);
            tensor<int32, [4]> var_179 = const()[name = tensor<string, []>("op_179"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_19_cast = reshape(shape = var_179, x = x_17_cast);
            tensor<int32, [4]> var_181 = const()[name = tensor<string, []>("op_181"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_185 = const()[name = tensor<string, []>("op_185"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_23_cast = reshape(shape = var_185, x = x_21_cast);
            tensor<bool, []> attention_scores_5_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_5_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_5_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_8_perm_0 = const()[name = tensor<string, []>("transpose_8_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_9_perm_0 = const()[name = tensor<string, []>("transpose_9_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp16, [1, 12, 32, 512]> transpose_35 = transpose(perm = transpose_9_perm_0, x = x_15_cast);
            tensor<fp16, [1, 12, 512, 32]> transpose_36 = transpose(perm = transpose_8_perm_0, x = x_23_cast);
            tensor<fp16, [1, 12, 512, 512]> attention_scores_5_cast = matmul(transpose_x = attention_scores_5_transpose_x_0, transpose_y = attention_scores_5_transpose_y_0, x = transpose_36, y = transpose_35);
            tensor<fp16, []> _inversed_attention_scores_7_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_attention_scores_7_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 512, 512]> _inversed_attention_scores_7_cast = mul(x = attention_scores_5_cast, y = _inversed_attention_scores_7_y_0_to_fp16);
            tensor<fp16, [1, 12, 512, 512]> input_33_cast = add(x = _inversed_attention_scores_7_cast, y = cast_193);
            tensor<fp16, [1, 12, 512, 512]> input_35_cast = softmax(axis = var_8, x = input_33_cast);
            tensor<bool, []> context_layer_5_transpose_x_0 = const()[name = tensor<string, []>("context_layer_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_5_transpose_y_0 = const()[name = tensor<string, []>("context_layer_5_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> transpose_37 = transpose(perm = var_181, x = x_19_cast);
            tensor<fp16, [1, 12, 512, 32]> context_layer_5_cast = matmul(transpose_x = context_layer_5_transpose_x_0, transpose_y = context_layer_5_transpose_y_0, x = input_35_cast, y = transpose_37);
            tensor<int32, [4]> var_197 = const()[name = tensor<string, []>("op_197"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_202 = const()[name = tensor<string, []>("op_202"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> transpose_34 = transpose(perm = var_197, x = context_layer_5_cast);
            tensor<fp16, [1, 512, 384]> input_37_cast = reshape(shape = var_202, x = transpose_34);
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28666624)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28961600)))];
            tensor<fp16, [1, 512, 384]> input_39_cast = linear(bias = model_encoder_layer_1_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_1_attention_output_dense_weight_to_fp16, x = input_37_cast);
            tensor<fp16, [1, 512, 384]> input_41_cast = add(x = input_39_cast, y = input_31_cast);
            tensor<int32, [1]> input_43_axes_0 = const()[name = tensor<string, []>("input_43_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28962432)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28963264)))];
            tensor<fp16, [1, 512, 384]> input_43_cast = layer_norm(axes = input_43_axes_0, beta = model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16, x = input_41_cast);
            tensor<fp16, [1536, 384]> model_encoder_layer_1_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28964096)))];
            tensor<fp16, [1536]> model_encoder_layer_1_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30143808)))];
            tensor<fp16, [1, 512, 1536]> input_45_cast = linear(bias = model_encoder_layer_1_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_1_intermediate_dense_weight_to_fp16, x = input_43_cast);
            tensor<string, []> input_47_mode_0 = const()[name = tensor<string, []>("input_47_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_47_cast = gelu(mode = input_47_mode_0, x = input_45_cast);
            tensor<fp16, [384, 1536]> model_encoder_layer_1_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30146944)))];
            tensor<fp16, [384]> model_encoder_layer_1_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31326656)))];
            tensor<fp16, [1, 512, 384]> input_49_cast = linear(bias = model_encoder_layer_1_output_dense_bias_to_fp16, weight = model_encoder_layer_1_output_dense_weight_to_fp16, x = input_47_cast);
            tensor<fp16, [1, 512, 384]> input_51_cast = add(x = input_49_cast, y = input_43_cast);
            tensor<int32, [1]> input_53_axes_0 = const()[name = tensor<string, []>("input_53_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_1_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31327488)))];
            tensor<fp16, [384]> model_encoder_layer_1_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31328320)))];
            tensor<fp16, [1, 512, 384]> input_53_cast = layer_norm(axes = input_53_axes_0, beta = model_encoder_layer_1_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_1_output_LayerNorm_weight_to_fp16, x = input_51_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31329152)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31624128)))];
            tensor<fp16, [1, 512, 384]> x_33_cast = linear(bias = model_encoder_layer_2_attention_self_query_bias_to_fp16, weight = model_encoder_layer_2_attention_self_query_weight_to_fp16, x = input_53_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31624960)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31919936)))];
            tensor<fp16, [1, 512, 384]> x_25_cast = linear(bias = model_encoder_layer_2_attention_self_key_bias_to_fp16, weight = model_encoder_layer_2_attention_self_key_weight_to_fp16, x = input_53_cast);
            tensor<int32, [4]> var_247 = const()[name = tensor<string, []>("op_247"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_27_cast = reshape(shape = var_247, x = x_25_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31920768)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32215744)))];
            tensor<fp16, [1, 512, 384]> x_29_cast = linear(bias = model_encoder_layer_2_attention_self_value_bias_to_fp16, weight = model_encoder_layer_2_attention_self_value_weight_to_fp16, x = input_53_cast);
            tensor<int32, [4]> var_256 = const()[name = tensor<string, []>("op_256"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_31_cast = reshape(shape = var_256, x = x_29_cast);
            tensor<int32, [4]> var_258 = const()[name = tensor<string, []>("op_258"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_262 = const()[name = tensor<string, []>("op_262"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_35_cast = reshape(shape = var_262, x = x_33_cast);
            tensor<bool, []> attention_scores_9_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_9_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_9_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_10_perm_0 = const()[name = tensor<string, []>("transpose_10_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_11_perm_0 = const()[name = tensor<string, []>("transpose_11_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp16, [1, 12, 32, 512]> transpose_31 = transpose(perm = transpose_11_perm_0, x = x_27_cast);
            tensor<fp16, [1, 12, 512, 32]> transpose_32 = transpose(perm = transpose_10_perm_0, x = x_35_cast);
            tensor<fp16, [1, 12, 512, 512]> attention_scores_9_cast = matmul(transpose_x = attention_scores_9_transpose_x_0, transpose_y = attention_scores_9_transpose_y_0, x = transpose_32, y = transpose_31);
            tensor<fp16, []> _inversed_attention_scores_11_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_attention_scores_11_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 512, 512]> _inversed_attention_scores_11_cast = mul(x = attention_scores_9_cast, y = _inversed_attention_scores_11_y_0_to_fp16);
            tensor<fp16, [1, 12, 512, 512]> input_55_cast = add(x = _inversed_attention_scores_11_cast, y = cast_193);
            tensor<fp16, [1, 12, 512, 512]> input_57_cast = softmax(axis = var_8, x = input_55_cast);
            tensor<bool, []> context_layer_9_transpose_x_0 = const()[name = tensor<string, []>("context_layer_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_9_transpose_y_0 = const()[name = tensor<string, []>("context_layer_9_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> transpose_33 = transpose(perm = var_258, x = x_31_cast);
            tensor<fp16, [1, 12, 512, 32]> context_layer_9_cast = matmul(transpose_x = context_layer_9_transpose_x_0, transpose_y = context_layer_9_transpose_y_0, x = input_57_cast, y = transpose_33);
            tensor<int32, [4]> var_274 = const()[name = tensor<string, []>("op_274"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_279 = const()[name = tensor<string, []>("op_279"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> transpose_30 = transpose(perm = var_274, x = context_layer_9_cast);
            tensor<fp16, [1, 512, 384]> input_59_cast = reshape(shape = var_279, x = transpose_30);
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32216576)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32511552)))];
            tensor<fp16, [1, 512, 384]> input_61_cast = linear(bias = model_encoder_layer_2_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_2_attention_output_dense_weight_to_fp16, x = input_59_cast);
            tensor<fp16, [1, 512, 384]> input_63_cast = add(x = input_61_cast, y = input_53_cast);
            tensor<int32, [1]> input_65_axes_0 = const()[name = tensor<string, []>("input_65_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32512384)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32513216)))];
            tensor<fp16, [1, 512, 384]> input_65_cast = layer_norm(axes = input_65_axes_0, beta = model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16, x = input_63_cast);
            tensor<fp16, [1536, 384]> model_encoder_layer_2_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32514048)))];
            tensor<fp16, [1536]> model_encoder_layer_2_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33693760)))];
            tensor<fp16, [1, 512, 1536]> input_67_cast = linear(bias = model_encoder_layer_2_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_2_intermediate_dense_weight_to_fp16, x = input_65_cast);
            tensor<string, []> input_69_mode_0 = const()[name = tensor<string, []>("input_69_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_69_cast = gelu(mode = input_69_mode_0, x = input_67_cast);
            tensor<fp16, [384, 1536]> model_encoder_layer_2_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33696896)))];
            tensor<fp16, [384]> model_encoder_layer_2_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34876608)))];
            tensor<fp16, [1, 512, 384]> input_71_cast = linear(bias = model_encoder_layer_2_output_dense_bias_to_fp16, weight = model_encoder_layer_2_output_dense_weight_to_fp16, x = input_69_cast);
            tensor<fp16, [1, 512, 384]> input_73_cast = add(x = input_71_cast, y = input_65_cast);
            tensor<int32, [1]> input_75_axes_0 = const()[name = tensor<string, []>("input_75_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_2_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34877440)))];
            tensor<fp16, [384]> model_encoder_layer_2_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34878272)))];
            tensor<fp16, [1, 512, 384]> input_75_cast = layer_norm(axes = input_75_axes_0, beta = model_encoder_layer_2_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_2_output_LayerNorm_weight_to_fp16, x = input_73_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34879104)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35174080)))];
            tensor<fp16, [1, 512, 384]> x_45_cast = linear(bias = model_encoder_layer_3_attention_self_query_bias_to_fp16, weight = model_encoder_layer_3_attention_self_query_weight_to_fp16, x = input_75_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35174912)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35469888)))];
            tensor<fp16, [1, 512, 384]> x_37_cast = linear(bias = model_encoder_layer_3_attention_self_key_bias_to_fp16, weight = model_encoder_layer_3_attention_self_key_weight_to_fp16, x = input_75_cast);
            tensor<int32, [4]> var_324 = const()[name = tensor<string, []>("op_324"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_39_cast = reshape(shape = var_324, x = x_37_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35470720)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35765696)))];
            tensor<fp16, [1, 512, 384]> x_41_cast = linear(bias = model_encoder_layer_3_attention_self_value_bias_to_fp16, weight = model_encoder_layer_3_attention_self_value_weight_to_fp16, x = input_75_cast);
            tensor<int32, [4]> var_333 = const()[name = tensor<string, []>("op_333"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_43_cast = reshape(shape = var_333, x = x_41_cast);
            tensor<int32, [4]> var_335 = const()[name = tensor<string, []>("op_335"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_339 = const()[name = tensor<string, []>("op_339"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_47_cast = reshape(shape = var_339, x = x_45_cast);
            tensor<bool, []> attention_scores_13_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_13_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_13_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_13_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_12_perm_0 = const()[name = tensor<string, []>("transpose_12_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_13_perm_0 = const()[name = tensor<string, []>("transpose_13_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp16, [1, 12, 32, 512]> transpose_27 = transpose(perm = transpose_13_perm_0, x = x_39_cast);
            tensor<fp16, [1, 12, 512, 32]> transpose_28 = transpose(perm = transpose_12_perm_0, x = x_47_cast);
            tensor<fp16, [1, 12, 512, 512]> attention_scores_13_cast = matmul(transpose_x = attention_scores_13_transpose_x_0, transpose_y = attention_scores_13_transpose_y_0, x = transpose_28, y = transpose_27);
            tensor<fp16, []> _inversed_attention_scores_15_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_attention_scores_15_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 512, 512]> _inversed_attention_scores_15_cast = mul(x = attention_scores_13_cast, y = _inversed_attention_scores_15_y_0_to_fp16);
            tensor<fp16, [1, 12, 512, 512]> input_77_cast = add(x = _inversed_attention_scores_15_cast, y = cast_193);
            tensor<fp16, [1, 12, 512, 512]> input_79_cast = softmax(axis = var_8, x = input_77_cast);
            tensor<bool, []> context_layer_13_transpose_x_0 = const()[name = tensor<string, []>("context_layer_13_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_13_transpose_y_0 = const()[name = tensor<string, []>("context_layer_13_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> transpose_29 = transpose(perm = var_335, x = x_43_cast);
            tensor<fp16, [1, 12, 512, 32]> context_layer_13_cast = matmul(transpose_x = context_layer_13_transpose_x_0, transpose_y = context_layer_13_transpose_y_0, x = input_79_cast, y = transpose_29);
            tensor<int32, [4]> var_351 = const()[name = tensor<string, []>("op_351"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_356 = const()[name = tensor<string, []>("op_356"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> transpose_26 = transpose(perm = var_351, x = context_layer_13_cast);
            tensor<fp16, [1, 512, 384]> input_81_cast = reshape(shape = var_356, x = transpose_26);
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35766528)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36061504)))];
            tensor<fp16, [1, 512, 384]> input_83_cast = linear(bias = model_encoder_layer_3_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_3_attention_output_dense_weight_to_fp16, x = input_81_cast);
            tensor<fp16, [1, 512, 384]> input_85_cast = add(x = input_83_cast, y = input_75_cast);
            tensor<int32, [1]> input_87_axes_0 = const()[name = tensor<string, []>("input_87_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36062336)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36063168)))];
            tensor<fp16, [1, 512, 384]> input_87_cast = layer_norm(axes = input_87_axes_0, beta = model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16, x = input_85_cast);
            tensor<fp16, [1536, 384]> model_encoder_layer_3_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36064000)))];
            tensor<fp16, [1536]> model_encoder_layer_3_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37243712)))];
            tensor<fp16, [1, 512, 1536]> input_89_cast = linear(bias = model_encoder_layer_3_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_3_intermediate_dense_weight_to_fp16, x = input_87_cast);
            tensor<string, []> input_91_mode_0 = const()[name = tensor<string, []>("input_91_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_91_cast = gelu(mode = input_91_mode_0, x = input_89_cast);
            tensor<fp16, [384, 1536]> model_encoder_layer_3_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37246848)))];
            tensor<fp16, [384]> model_encoder_layer_3_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38426560)))];
            tensor<fp16, [1, 512, 384]> input_93_cast = linear(bias = model_encoder_layer_3_output_dense_bias_to_fp16, weight = model_encoder_layer_3_output_dense_weight_to_fp16, x = input_91_cast);
            tensor<fp16, [1, 512, 384]> input_95_cast = add(x = input_93_cast, y = input_87_cast);
            tensor<int32, [1]> input_97_axes_0 = const()[name = tensor<string, []>("input_97_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_3_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38427392)))];
            tensor<fp16, [384]> model_encoder_layer_3_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38428224)))];
            tensor<fp16, [1, 512, 384]> input_97_cast = layer_norm(axes = input_97_axes_0, beta = model_encoder_layer_3_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_3_output_LayerNorm_weight_to_fp16, x = input_95_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38429056)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38724032)))];
            tensor<fp16, [1, 512, 384]> x_57_cast = linear(bias = model_encoder_layer_4_attention_self_query_bias_to_fp16, weight = model_encoder_layer_4_attention_self_query_weight_to_fp16, x = input_97_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38724864)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39019840)))];
            tensor<fp16, [1, 512, 384]> x_49_cast = linear(bias = model_encoder_layer_4_attention_self_key_bias_to_fp16, weight = model_encoder_layer_4_attention_self_key_weight_to_fp16, x = input_97_cast);
            tensor<int32, [4]> var_401 = const()[name = tensor<string, []>("op_401"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_51_cast = reshape(shape = var_401, x = x_49_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39020672)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39315648)))];
            tensor<fp16, [1, 512, 384]> x_53_cast = linear(bias = model_encoder_layer_4_attention_self_value_bias_to_fp16, weight = model_encoder_layer_4_attention_self_value_weight_to_fp16, x = input_97_cast);
            tensor<int32, [4]> var_410 = const()[name = tensor<string, []>("op_410"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_55_cast = reshape(shape = var_410, x = x_53_cast);
            tensor<int32, [4]> var_412 = const()[name = tensor<string, []>("op_412"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_416 = const()[name = tensor<string, []>("op_416"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_59_cast = reshape(shape = var_416, x = x_57_cast);
            tensor<bool, []> attention_scores_17_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_17_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_17_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_17_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_14_perm_0 = const()[name = tensor<string, []>("transpose_14_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_15_perm_0 = const()[name = tensor<string, []>("transpose_15_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp16, [1, 12, 32, 512]> transpose_23 = transpose(perm = transpose_15_perm_0, x = x_51_cast);
            tensor<fp16, [1, 12, 512, 32]> transpose_24 = transpose(perm = transpose_14_perm_0, x = x_59_cast);
            tensor<fp16, [1, 12, 512, 512]> attention_scores_17_cast = matmul(transpose_x = attention_scores_17_transpose_x_0, transpose_y = attention_scores_17_transpose_y_0, x = transpose_24, y = transpose_23);
            tensor<fp16, []> _inversed_attention_scores_19_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_attention_scores_19_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 512, 512]> _inversed_attention_scores_19_cast = mul(x = attention_scores_17_cast, y = _inversed_attention_scores_19_y_0_to_fp16);
            tensor<fp16, [1, 12, 512, 512]> input_99_cast = add(x = _inversed_attention_scores_19_cast, y = cast_193);
            tensor<fp16, [1, 12, 512, 512]> input_101_cast = softmax(axis = var_8, x = input_99_cast);
            tensor<bool, []> context_layer_17_transpose_x_0 = const()[name = tensor<string, []>("context_layer_17_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_17_transpose_y_0 = const()[name = tensor<string, []>("context_layer_17_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> transpose_25 = transpose(perm = var_412, x = x_55_cast);
            tensor<fp16, [1, 12, 512, 32]> context_layer_17_cast = matmul(transpose_x = context_layer_17_transpose_x_0, transpose_y = context_layer_17_transpose_y_0, x = input_101_cast, y = transpose_25);
            tensor<int32, [4]> var_428 = const()[name = tensor<string, []>("op_428"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_433 = const()[name = tensor<string, []>("op_433"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> transpose_22 = transpose(perm = var_428, x = context_layer_17_cast);
            tensor<fp16, [1, 512, 384]> input_103_cast = reshape(shape = var_433, x = transpose_22);
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39316480)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39611456)))];
            tensor<fp16, [1, 512, 384]> input_105_cast = linear(bias = model_encoder_layer_4_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_4_attention_output_dense_weight_to_fp16, x = input_103_cast);
            tensor<fp16, [1, 512, 384]> input_107_cast = add(x = input_105_cast, y = input_97_cast);
            tensor<int32, [1]> input_109_axes_0 = const()[name = tensor<string, []>("input_109_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39612288)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39613120)))];
            tensor<fp16, [1, 512, 384]> input_109_cast = layer_norm(axes = input_109_axes_0, beta = model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16, x = input_107_cast);
            tensor<fp16, [1536, 384]> model_encoder_layer_4_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39613952)))];
            tensor<fp16, [1536]> model_encoder_layer_4_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40793664)))];
            tensor<fp16, [1, 512, 1536]> input_111_cast = linear(bias = model_encoder_layer_4_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_4_intermediate_dense_weight_to_fp16, x = input_109_cast);
            tensor<string, []> input_113_mode_0 = const()[name = tensor<string, []>("input_113_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_113_cast = gelu(mode = input_113_mode_0, x = input_111_cast);
            tensor<fp16, [384, 1536]> model_encoder_layer_4_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40796800)))];
            tensor<fp16, [384]> model_encoder_layer_4_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41976512)))];
            tensor<fp16, [1, 512, 384]> input_115_cast = linear(bias = model_encoder_layer_4_output_dense_bias_to_fp16, weight = model_encoder_layer_4_output_dense_weight_to_fp16, x = input_113_cast);
            tensor<fp16, [1, 512, 384]> input_117_cast = add(x = input_115_cast, y = input_109_cast);
            tensor<int32, [1]> input_119_axes_0 = const()[name = tensor<string, []>("input_119_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_4_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41977344)))];
            tensor<fp16, [384]> model_encoder_layer_4_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41978176)))];
            tensor<fp16, [1, 512, 384]> input_119_cast = layer_norm(axes = input_119_axes_0, beta = model_encoder_layer_4_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_4_output_LayerNorm_weight_to_fp16, x = input_117_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41979008)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42273984)))];
            tensor<fp16, [1, 512, 384]> x_69_cast = linear(bias = model_encoder_layer_5_attention_self_query_bias_to_fp16, weight = model_encoder_layer_5_attention_self_query_weight_to_fp16, x = input_119_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42274816)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42569792)))];
            tensor<fp16, [1, 512, 384]> x_61_cast = linear(bias = model_encoder_layer_5_attention_self_key_bias_to_fp16, weight = model_encoder_layer_5_attention_self_key_weight_to_fp16, x = input_119_cast);
            tensor<int32, [4]> var_478 = const()[name = tensor<string, []>("op_478"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_63_cast = reshape(shape = var_478, x = x_61_cast);
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42570624)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42865600)))];
            tensor<fp16, [1, 512, 384]> x_65_cast = linear(bias = model_encoder_layer_5_attention_self_value_bias_to_fp16, weight = model_encoder_layer_5_attention_self_value_weight_to_fp16, x = input_119_cast);
            tensor<int32, [4]> var_487 = const()[name = tensor<string, []>("op_487"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_67_cast = reshape(shape = var_487, x = x_65_cast);
            tensor<int32, [4]> var_489 = const()[name = tensor<string, []>("op_489"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_493 = const()[name = tensor<string, []>("op_493"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_cast = reshape(shape = var_493, x = x_69_cast);
            tensor<bool, []> attention_scores_21_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_21_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_21_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_21_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_16_perm_0 = const()[name = tensor<string, []>("transpose_16_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_17_perm_0 = const()[name = tensor<string, []>("transpose_17_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp16, [1, 12, 32, 512]> transpose_19 = transpose(perm = transpose_17_perm_0, x = x_63_cast);
            tensor<fp16, [1, 12, 512, 32]> transpose_20 = transpose(perm = transpose_16_perm_0, x = x_cast);
            tensor<fp16, [1, 12, 512, 512]> attention_scores_21_cast = matmul(transpose_x = attention_scores_21_transpose_x_0, transpose_y = attention_scores_21_transpose_y_0, x = transpose_20, y = transpose_19);
            tensor<fp16, []> _inversed_attention_scores_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_attention_scores_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 512, 512]> _inversed_attention_scores_cast = mul(x = attention_scores_21_cast, y = _inversed_attention_scores_y_0_to_fp16);
            tensor<string, []> attention_mask_to_fp16_dtype_1 = const()[name = tensor<string, []>("attention_mask_to_fp16_dtype_1"), val = tensor<string, []>("fp16")];
            tensor<fp16, [1, 1, 1, 512]> cast_53 = cast(dtype = attention_mask_to_fp16_dtype_1, x = attention_mask_1);
            tensor<fp16, [1, 12, 512, 512]> input_121_cast = add(x = _inversed_attention_scores_cast, y = cast_53);
            tensor<fp16, [1, 12, 512, 512]> input_123_cast = softmax(axis = var_8, x = input_121_cast);
            tensor<bool, []> context_layer_21_transpose_x_0 = const()[name = tensor<string, []>("context_layer_21_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_21_transpose_y_0 = const()[name = tensor<string, []>("context_layer_21_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> transpose_21 = transpose(perm = var_489, x = x_67_cast);
            tensor<fp16, [1, 12, 512, 32]> context_layer_21_cast = matmul(transpose_x = context_layer_21_transpose_x_0, transpose_y = context_layer_21_transpose_y_0, x = input_123_cast, y = transpose_21);
            tensor<int32, [4]> var_505 = const()[name = tensor<string, []>("op_505"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_510 = const()[name = tensor<string, []>("op_510"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> transpose_18 = transpose(perm = var_505, x = context_layer_21_cast);
            tensor<fp16, [1, 512, 384]> input_125_cast = reshape(shape = var_510, x = transpose_18);
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42866432)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43161408)))];
            tensor<fp16, [1, 512, 384]> input_127_cast = linear(bias = model_encoder_layer_5_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_5_attention_output_dense_weight_to_fp16, x = input_125_cast);
            tensor<fp16, [1, 512, 384]> input_129_cast = add(x = input_127_cast, y = input_119_cast);
            tensor<int32, [1]> input_131_axes_0 = const()[name = tensor<string, []>("input_131_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43162240)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43163072)))];
            tensor<fp16, [1, 512, 384]> input_131_cast = layer_norm(axes = input_131_axes_0, beta = model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16, x = input_129_cast);
            tensor<fp16, [1536, 384]> model_encoder_layer_5_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43163904)))];
            tensor<fp16, [1536]> model_encoder_layer_5_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(44343616)))];
            tensor<fp16, [1, 512, 1536]> input_133_cast = linear(bias = model_encoder_layer_5_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_5_intermediate_dense_weight_to_fp16, x = input_131_cast);
            tensor<string, []> input_135_mode_0 = const()[name = tensor<string, []>("input_135_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_135_cast = gelu(mode = input_135_mode_0, x = input_133_cast);
            tensor<fp16, [384, 1536]> model_encoder_layer_5_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(44346752)))];
            tensor<fp16, [384]> model_encoder_layer_5_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45526464)))];
            tensor<fp16, [1, 512, 384]> input_137_cast = linear(bias = model_encoder_layer_5_output_dense_bias_to_fp16, weight = model_encoder_layer_5_output_dense_weight_to_fp16, x = input_135_cast);
            tensor<fp16, [1, 512, 384]> input_139_cast = add(x = input_137_cast, y = input_131_cast);
            tensor<int32, [1]> hidden_states_axes_0 = const()[name = tensor<string, []>("hidden_states_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_5_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45527296)))];
            tensor<fp16, [384]> model_encoder_layer_5_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45528128)))];
            tensor<fp16, [1, 512, 384]> hidden_states_cast = layer_norm(axes = hidden_states_axes_0, beta = model_encoder_layer_5_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = model_encoder_layer_5_output_LayerNorm_weight_to_fp16, x = input_139_cast);
            tensor<int32, [3]> var_546_begin_0 = const()[name = tensor<string, []>("op_546_begin_0"), val = tensor<int32, [3]>([0, 0, 0])];
            tensor<int32, [3]> var_546_end_0 = const()[name = tensor<string, []>("op_546_end_0"), val = tensor<int32, [3]>([1, 1, 384])];
            tensor<bool, [3]> var_546_end_mask_0 = const()[name = tensor<string, []>("op_546_end_mask_0"), val = tensor<bool, [3]>([true, false, true])];
            tensor<bool, [3]> var_546_squeeze_mask_0 = const()[name = tensor<string, []>("op_546_squeeze_mask_0"), val = tensor<bool, [3]>([false, true, false])];
            tensor<fp16, [1, 384]> var_546_cast = slice_by_index(begin = var_546_begin_0, end = var_546_end_0, end_mask = var_546_end_mask_0, squeeze_mask = var_546_squeeze_mask_0, x = hidden_states_cast);
            tensor<string, []> var_546_cast_to_fp32_dtype_0 = const()[name = tensor<string, []>("op_546_cast_to_fp32_dtype_0"), val = tensor<string, []>("fp32")];
            tensor<fp32, [1, 384]> embeddings = cast(dtype = var_546_cast_to_fp32_dtype_0, x = var_546_cast);
        } -> (embeddings);
}